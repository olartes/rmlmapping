{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "VENDOR_PATH = path.join('vendor')\n",
    "TMP_PATH = path.join('tmp')\n",
    "\n",
    "DATA_PATH = path.join('data')\n",
    "DATA_ORIGIN_PATH = path.join('data_originals')\n",
    "\n",
    "RML_PATH = path.join('rml')\n",
    "RML_INCLUDES_PATH = path.join(RML_PATH, 'includes')\n",
    "RML_RULES_PATH = path.join(RML_PATH, 'rules')\n",
    "RML_MAPPER_JAR = path.join(VENDOR_PATH, 'rmlmapper-7.0.0-r374-all.jar')\n",
    "\n",
    "OUTPUT_DIR = path.join('output')\n",
    "\n",
    "RED='\\033[0;31m'\n",
    "GREEN='\\033[0;32m'\n",
    "NC='\\033[0m' # No Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import mkdir\n",
    "\n",
    "if not path.exists(DATA_PATH):\n",
    "  mkdir(DATA_PATH)\n",
    "\n",
    "if not path.exists(DATA_ORIGIN_PATH):\n",
    "  mkdir(DATA_ORIGIN_PATH)\n",
    "\n",
    "if not path.exists(VENDOR_PATH):\n",
    "  mkdir(VENDOR_PATH)\n",
    "\n",
    "if not path.exists(TMP_PATH):\n",
    "  mkdir(TMP_PATH)\n",
    "\n",
    "if not path.exists(OUTPUT_DIR):\n",
    "  mkdir(OUTPUT_DIR)\n",
    "\n",
    "if not path.exists(TMP_PATH):\n",
    "  mkdir(TMP_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from ipfs_cid import cid_sha256_hash\n",
    "\n",
    "#\n",
    "# TODO comment\n",
    "\n",
    "LB3000_CSV = path.join(DATA_ORIGIN_PATH, 'okuma_lb3000.csv')\n",
    "_, lb3000_filename = path.split(LB3000_CSV)\n",
    "\n",
    "\n",
    "#\n",
    "# TODO comment\n",
    "\n",
    "mtools_filename = lb3000_filename.replace('.', '.mtools.')\n",
    "LB3000_MTOOLS_CSV = path.join(DATA_PATH, mtools_filename)\n",
    "\n",
    "components_filename = lb3000_filename.replace('.', '.components.')\n",
    "LB3000_COMPONENTS_CSV = path.join(DATA_PATH, components_filename)\n",
    "\n",
    "\n",
    "#\n",
    "# TODO comment\n",
    "\n",
    "required_filename = lb3000_filename.replace('.', '.required.')\n",
    "LB3000_REQUIRED_CSV = path.join(DATA_PATH, required_filename)\n",
    "\n",
    "recommended_filename = lb3000_filename.replace('.', '.recommended.')\n",
    "LB3000_RECOMMENDED_CSV = path.join(DATA_PATH, recommended_filename)\n",
    "\n",
    "incompatible_filename = lb3000_filename.replace('.', '.incompatible.')\n",
    "LB3000_INCOMPATIBLE_CSV = path.join(DATA_PATH, incompatible_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMFILE_HEADERS = [\n",
    "  'pic_itemId', 'pic_itemCode',\n",
    "  'itm_name', 'itm_description', 'CEL_INVENTCLASSAID', 'FULLPATH',\n",
    "  'CID'\n",
    "]\n",
    "\n",
    "lb3000csv = pd.read_csv(LB3000_CSV)\n",
    "\n",
    "with open(LB3000_MTOOLS_CSV, 'w') as mtools_out, \\\n",
    "     open(LB3000_COMPONENTS_CSV, 'w') as components_out:\n",
    "\n",
    "  mtools_writer = csv.writer(mtools_out)\n",
    "  mtools_writer.writerow(ITEMFILE_HEADERS)\n",
    "\n",
    "  components_writer = csv.writer(components_out)\n",
    "  components_writer.writerow(ITEMFILE_HEADERS)\n",
    "\n",
    "  for index, row in lb3000csv.iterrows():\n",
    "\n",
    "    cid1 = cid_sha256_hash(str.encode(row['itm_name1']))\n",
    "    item1 = pd.concat([row[0:6], pd.Series([cid1])], ignore_index=True)\n",
    "    if row['itm_name1'].startswith('OKUMA'):\n",
    "      mtools_writer.writerow(item1)\n",
    "    else:\n",
    "      components_writer.writerow(item1)\n",
    "\n",
    "    cid2 = cid_sha256_hash(str.encode(row['itm_name2']))\n",
    "    item2 = pd.concat([row[7:13], pd.Series([cid2])], ignore_index=True)\n",
    "    # components_writer.writerow(item2)\n",
    "    if row['itm_name2'].startswith('OKUMA'):\n",
    "      mtools_writer.writerow(item2)\n",
    "    else:\n",
    "      components_writer.writerow(item2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMFILE_HEADERS_SORTED = [\n",
    "  'CID',\n",
    "  'itm_name', 'itm_description', 'CEL_INVENTCLASSAID', 'FULLPATH',\n",
    "  'pic_itemId', 'pic_itemCode'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(LB3000_MTOOLS_CSV)\n",
    "df.drop_duplicates(subset=['CID'], keep='first', inplace=True)\n",
    "df.reindex(columns=ITEMFILE_HEADERS_SORTED).to_csv(LB3000_MTOOLS_CSV, index=False)\n",
    "\n",
    "df = pd.read_csv(LB3000_COMPONENTS_CSV)\n",
    "df.drop_duplicates(subset=['CID'], keep='first', inplace=True)\n",
    "df.reindex(columns=ITEMFILE_HEADERS_SORTED).to_csv(LB3000_COMPONENTS_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTOOL_CLASS = 'MachineTool'\n",
    "COMPONENT_CLASS = 'Component'\n",
    "RELFILE_HEADERS = ['CLASSNAME1', 'CID1', 'CLASSNAME2', 'CID2']\n",
    "\n",
    "lb3000csv = pd.read_csv(LB3000_CSV)\n",
    "\n",
    "with open(LB3000_REQUIRED_CSV, 'w') as required_out, \\\n",
    "     open(LB3000_RECOMMENDED_CSV, 'w') as recommended_out, \\\n",
    "     open(LB3000_INCOMPATIBLE_CSV, 'w') as incompatible_out:\n",
    "\n",
    "  required_writer = csv.writer(required_out)\n",
    "  required_writer.writerow(RELFILE_HEADERS)\n",
    "\n",
    "  recommended_writer = csv.writer(recommended_out)\n",
    "  recommended_writer.writerow(RELFILE_HEADERS)\n",
    "\n",
    "  incompatible_writer = csv.writer(incompatible_out)\n",
    "  incompatible_writer.writerow(RELFILE_HEADERS)\n",
    "\n",
    "  for index, row in lb3000csv.iterrows():\n",
    "    cid1 = cid_sha256_hash(str.encode(row['itm_name1']))\n",
    "    class1 = MTOOL_CLASS if row['itm_name1'].startswith('OKUMA') else COMPONENT_CLASS\n",
    "\n",
    "    cid2 = cid_sha256_hash(str.encode(row['itm_name2']))\n",
    "    class2 = MTOOL_CLASS if row['itm_name2'].startswith('OKUMA') else COMPONENT_CLASS\n",
    "\n",
    "    if row['RELATION'] == 'Required':\n",
    "      required_writer.writerow([class1, cid1, class2, cid2])\n",
    "    elif row['RELATION'] == 'Recommended':\n",
    "      recommended_writer.writerow([class1, cid1, class2, cid2])\n",
    "    elif row['RELATION'] == 'Incompatible':\n",
    "      incompatible_writer.writerow([class1, cid1, class2, cid2])\n",
    "    else:\n",
    "      print(f'{RED}Unknown relation: {row[\"RELATION\"]}{NC}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RML Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "MAPPING_STAGE1 = os.path.join(RML_PATH, 'mapping.stage1.ttl')\n",
    "MAPPING_STAGE2 = os.path.join(RML_PATH, 'mapping.stage2.ttl')\n",
    "MAPPING_TEMPLATE = os.path.join(RML_PATH, 'mapping.template.ttl')\n",
    "MAPPING_FILE = os.path.join(TMP_PATH, 'mapping.rml.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Collecting all rule files ....... \u001b[0;32mDONE\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RULE_FILE_PATH = path.join(RML_RULES_PATH, '**', '*.rule.ttl')\n",
    "rule_files = glob(RULE_FILE_PATH, recursive=True)\n",
    "rule_files.sort()\n",
    "\n",
    "print(\" * Collecting all rule files \", end='')\n",
    "with open(MAPPING_STAGE1, 'w') as bundle:\n",
    "  for rule_file in rule_files:\n",
    "    print('.', sep='', end='', flush=True)\n",
    "    with open(rule_file, 'r') as rule:\n",
    "      shutil.copyfileobj(rule, bundle)\n",
    "    bundle.write('\\n')\n",
    "  print(' ', sep='', end='', flush=True)\n",
    "print(f\"{GREEN}DONE{NC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Incorporate include files .. \u001b[0;32mDONE\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "INC_FILES_PATH = path.join(RML_INCLUDES_PATH, '**', '*.inc.ttl')\n",
    "incfiles = glob(INC_FILES_PATH, recursive=True)\n",
    "\n",
    "print(\" * Incorporate include files \", end='')\n",
    "with open(MAPPING_STAGE2, 'w') as bundle:\n",
    "  content = Path(MAPPING_STAGE1).read_text()\n",
    "  for incfile in incfiles:\n",
    "    print('.', sep='', end='', flush=True)\n",
    "    incfile = Path(incfile)\n",
    "    placeholder = f'###__{incfile.stem}__'\n",
    "    content = content.replace(placeholder, incfile.read_text())\n",
    "  bundle.write(content)\n",
    "  print(' ', sep='', end='', flush=True)\n",
    "print(f\"{GREEN}DONE{NC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rml\\\\mapping.template.ttl'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(MAPPING_STAGE1)\n",
    "#os.rename(MAPPING_STAGE2, MAPPING_TEMPLATE)\n",
    "shutil.move(MAPPING_STAGE2, MAPPING_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "\n",
    "### Requirements\n",
    "Install [rmlmapper](https://github.com/RMLio/rmlmapper-java/releases/tag/v7.0.0) jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_CSV = path.join(DATA_ORIGIN_PATH, 'classes.csv')\n",
    "KG_FILE = path.join(OUTPUT_DIR, lb3000_filename.replace('.csv', '.nt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "with open(MAPPING_TEMPLATE, 'r') as template_file:\n",
    "  content = template_file.read()\n",
    "  content = content.replace('__CLASSES_CSV__', CLASSES_CSV)\n",
    "  content = content.replace('__MTOOLS_SOURCE__', LB3000_MTOOLS_CSV)\n",
    "  content = content.replace('__COMPONENT_SOURCE__', LB3000_COMPONENTS_CSV)\n",
    "  content = content.replace('__REQUIRED_SOURCE__', LB3000_REQUIRED_CSV)\n",
    "  content = content.replace('__RECOMMENDED_SOURCE__', LB3000_RECOMMENDED_CSV)\n",
    "  content = content.replace('__INCOMPATIBLE_SOURCE__', LB3000_INCOMPATIBLE_CSV)\n",
    "\n",
    "  with open(MAPPING_FILE, 'w') as mapping_file:\n",
    "    mapping_file.write(content)\n",
    "\n",
    "subprocess.call([\n",
    "  'java', '-jar', RML_MAPPER_JAR,\n",
    "    '-m', MAPPING_FILE,\n",
    "    '-o', KG_FILE\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3_11-olartes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
